# FAA Agent - Development Session Summary

**Date:** January 9, 2026

## Overview

Built a multi-turn conversational AI agent for FAA certification guidance using Claude API orchestration, Azure AI Search with Cohere embeddings, and a progressive document caching system.

---

## Architecture Decisions

### 1. FastAPI over Azure Functions
**Decision:** Use FastAPI with WebSocket support instead of Azure Functions.

**Rationale:** WebSocket provides real-time streaming responses essential for the chat interface. Azure Functions has limited WebSocket support and would require additional complexity.

### 2. Cohere Embeddings (1024 dimensions)
**Decision:** Use Azure AI Services with Cohere-embed-v3-english model.

**Rationale:** Consistent with the user's existing `faa-search-web` project which already uses Cohere. Provides high-quality embeddings optimized for semantic search.

### 3. Hybrid Search (Keyword + Vector)
**Decision:** Combine keyword matching with vector similarity in Azure AI Search.

**Rationale:** FAA documents contain specific section numbers (§25.1309) and technical terms that benefit from exact matching, while conceptual questions benefit from semantic search.

### 4. Progressive Caching with Auto-Indexing
**Decision:** Cache documents on first fetch, index them on first cache hit.

**Rationale:** 
- First fetch → Store in cache (cold)
- Second fetch → Serve from cache + trigger background indexing
- Future fetches → Serve from cache + document now searchable

This makes the system self-improving without requiring manual curation.

### 5. Simple Background Tasks (asyncio.create_task)
**Decision:** Use `asyncio.create_task()` for background indexing instead of a durable queue.

**Rationale:** Acceptable trade-off for this use case. Tasks are lost on restart, but indexing is opportunistic—documents will be re-indexed on the next cache hit. Adding Redis/Celery would increase complexity without significant benefit.

### 6. Document Graph Walking via Prompt Engineering
**Decision:** Instruct Claude to follow document references via system prompt rather than implementing a dedicated reference extraction tool.

**Rationale:** Claude can already identify references in text (§25.1317, AC 20-136, etc.). A separate tool adds complexity without significant benefit. The enhanced system prompt guides Claude to proactively follow reference chains.

---

## Components Built

### Backend Services

| File | Purpose |
|------|---------|
| `app/services/orchestrator.py` | Claude API tool-calling loop with streaming |
| `app/services/cache.py` | Azure Blob Storage document cache |
| `app/services/indexer.py` | Background embedding generation + index upload |
| `app/services/conversation.py` | Conversation history management |

### Tools (Claude can call these)

| Tool | Purpose |
|------|---------|
| `search_indexed_content` | Hybrid search on Azure AI Search index |
| `fetch_cfr_section` | Fetch CFR text from eCFR API (with caching) |
| `search_drs` | Search FAA DRS for Advisory Circulars |
| `fetch_drs_document` | Fetch AC/AD documents with PDF extraction (with caching) |

### Infrastructure

| Resource | Name | Purpose |
|----------|------|---------|
| Azure AI Search | `faa-ai-search` | Vector + keyword search index |
| Azure AI Services | `faa-ai-services` | Cohere embedding model |
| Azure Blob Storage | `faaagentcache` | Document cache |
| Search Index | `faa-agent` | 1024-dim vectors, hybrid search |

---

## Key Flows

### Query Flow
```
User Question
    → Claude decides what tools to call
    → search_indexed_content (find relevant docs)
    → fetch_cfr_section / fetch_drs_document (get full text)
    → Claude identifies references, fetches those too
    → Claude synthesizes answer with citations
```

### Caching Flow
```
fetch_cfr_section("25.1309")
    → Check blob cache
    → MISS: Fetch from eCFR API → Store in cache → Return
    → HIT: Return cached content → Schedule background indexing
```

### Auto-Indexing Flow
```
Background task triggered on cache hit (if not already indexed):
    → Generate Cohere embedding
    → Upload to Azure AI Search
    → Mark document as indexed in cache metadata
```

---

## Index Schema

```
Fields:
  - id (string, key)
  - title (string, searchable)
  - content (string, searchable)
  - source (string)
  - doc_type (string, filterable)
  - citation (string)
  - embedding (vector, 1024 dimensions)
```

---

## System Prompt (Document Graph Walking)

Key instructions added to guide Claude:
1. Search indexed content first
2. Fetch complete text when needed
3. **Walk the document graph**: Look for citations like §25.1317, AC 20-136 and fetch them if relevant
4. Follow reference chains until complete context is gathered
5. Always cite sources with specific section numbers

Includes concrete example (HIRF question flow) showing how to chain: §25.1317 → §25.1309 → AC 20-158.

---

## Configuration

### Environment Variables
```env
# Anthropic
ANTHROPIC_API_KEY=...
ANTHROPIC_MODEL=claude-sonnet-4-5-20250929

# Azure AI Services (Cohere embeddings)
AZURE_AI_SERVICES_ENDPOINT=https://faa-ai-services.cognitiveservices.azure.com
AZURE_AI_SERVICES_KEY=...
AZURE_AI_SERVICES_EMBEDDING_DEPLOYMENT=cohere-embed

# Azure AI Search
AZURE_SEARCH_ENDPOINT=https://faa-ai-search.search.windows.net
AZURE_SEARCH_KEY=...
AZURE_SEARCH_INDEX=faa-agent

# Azure Blob Storage (cache)
AZURE_BLOB_CONNECTION_STRING=...
AZURE_BLOB_CONTAINER_NAME=documents

# Feature Flags
CACHE_ENABLED=true
AUTO_INDEX_ON_CACHE_HIT=true

# External APIs
DRS_API_KEY=...
```

---

## Testing Results

### Stall Speed Question
- Claude searched index, fetched §25.103, §23.2110, AC 25-7D, AC 23-8C
- All documents cached on first run
- Second run: cache hits, background indexing triggered
- §25.103 now appears in search results

### HIRF Question
- Claude followed document graph: §25.1317 → §25.1309 → AC 20-158B
- Provided comprehensive tiered protection requirements
- Cited all sources correctly

---

## Current Index State

- **22 documents indexed** (21 seeded + 1 auto-indexed)
- **8 documents cached** (CFR sections + Advisory Circulars)

---

## Future Considerations

1. **Frontend**: SolidJS chat interface (not started)
2. **PostgreSQL**: Conversation persistence (stubbed, not connected)
3. **Deployment**: Azure App Service + Static Web Apps
4. **More robust queuing**: If indexing reliability becomes critical, consider Azure Queue Storage
5. **Index expansion**: Could pre-seed more sections or rely entirely on progressive indexing
